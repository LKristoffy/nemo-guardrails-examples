models:
  - type: main
    engine: openai
    model: gpt-3.5-turbo
    parameters:
      temperature: 0.0    

  # Using Nvidia API  
  # - type: content_safety
  #   engine: openai
  #   model: nvidia/llama-3.1-nemoguard-8b-content-safety
  # Using local LLM with content safety model, must start the server first with something like ollama
  - type: llama_guard
    engine: openai
    parameters:
      openai_api_base: "http://localhost:11434/v1"
      model_name: "llama-guard3:8b"   

rails:
  input:
    flows:
      - mask sensitive data on input     # Presidio (no LLM)
      # - injection detection # If no end-point given, it will run on-device
      # - content safety check input $model=content_safety
      - content safety check input $model=llama_guard
      - self check input
      - jailbreak detection heuristics
  output:
    flows:
      - self check hallucination
  config:
    sensitive_data_detection:            # Presidio tuning
      input:
        entities: [CREDIT_CARD, CRYPTO, EMAIL_ADDRESS, IBAN_CODE, PHONE_NUMBER, MEDICAL_LICENSE, UK_NHS, UK_NINO, US_BANK_NUMBER, US_DRIVER_LICENSE, US_PASSPORT, US_SSN]
      output:
        entities: [CREDIT_CARD, CRYPTO, EMAIL_ADDRESS, IBAN_CODE, PHONE_NUMBER, MEDICAL_LICENSE, UK_NHS, UK_NINO, US_BANK_NUMBER, US_DRIVER_LICENSE, US_PASSPORT, US_SSN]
    jailbreak_detection:
      #server_endpoint: "http://0.0.0.0:1337/heuristics"
      length_per_perplexity_threshold: 89.79
      prefix_suffix_perplexity_threshold: 1845.65

tracing:
  enabled: true
  adapters:
    - name: OpenTelemetry
      service_name: "nemo_guardrails_service"
      exporter: "zipkin"  # Options: "console", "zipkin", etc.
      resource_attributes:
        env: "production"
    - name: FileSystem
      filepath: './traces/traces.jsonl'

